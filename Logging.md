# Logging



## Strategy

- It is beneficial to use an UUID to denote a transaction process, so that it will be more clear for developer to check the operation for a single process.
- print is not a good idea, because compared with logging functions:
  - You can not control message level and filter out not important ones
  - You can not decide where and how to output later


The id used to link system:

- `correlationId`: a code which is sent across systems, from very beginning of an operation to final return values to customers. In Http request's header, it goes like this `X-Correlation-ID`
- `logId`: a code which record the progress of a function. 

**Correlation IDs**

- pass it along in the HTTP header to every other service
- include it in every log message

**SERVICE INTERACTIONS**

A single user request can result in activity occurring across many services, which makes things difficult when trying to debug the impact of a specific request. One way to make things simpler is to include a correlation ID in service requests. A correlation ID is a unique identifier for the originating request that is passed by each service to any downstream requests. When combined with a [centralized logging](http://www.vinaysahni.com/best-practices-for-building-a-microservice-architecture#logging) layer, this makes it really easy to see a request make it's way through your infrastructure.

The IDs are generated by either user facing [aggregation service](http://www.vinaysahni.com/best-practices-for-building-a-microservice-architecture#aggregation-services) or by any service that needs to make a request that's not an immediate side effect of an incoming request. Any sufficiently random string (like a [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier)) would do the trick.


## Language

### Python

`logging` 提供root级别的log

`logger` 提供root级别下，特定element 级别的访问

```python
import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
```

 By giving different level to logger or handler, you can write only error messages to specific log file, or record debug details when debugging. 



**Logger Name**

`logger = logging.getLogger(__name__)`



**Capture exceptions and record them with traceback**

By calling logger methods with exc_info=True parameter, traceback is dumped to the logger

```python
try:
    open('/path/to/does/not/exist', 'rb')
except (SystemExit, KeyboardInterrupt):
    raise
except Exception, e:
    logger.error('Failed to open file', exc_info=True)
```



**Get logger when need it**

It’s better to get the logger when you need it. It’s cheap to create or get a logger. 



**Use JSON or YAML logging configuration**

``` yaml
version: 1
disable_existing_loggers: False
formatters:
    simple:
        format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

handlers:
    console:
        class: logging.StreamHandler
        level: DEBUG
        formatter: simple
        stream: ext://sys.stdout

    info_file_handler:
        class: logging.handlers.RotatingFileHandler
        level: INFO
        formatter: simple
        filename: info.log
        maxBytes: 10485760 # 10MB
        backupCount: 20
        encoding: utf8

    error_file_handler:
        class: logging.handlers.RotatingFileHandler
        level: ERROR
        formatter: simple
        filename: errors.log
        maxBytes: 10485760 # 10MB
        backupCount: 20
        encoding: utf8

loggers:
    my_module:
        level: ERROR
        handlers: [console]
        propagate: no

root:
    level: INFO
    handlers: [console, info_file_handler, error_file_handler]
```



``` python
import os
import logging.config

import yaml

def setup_logging(
    default_path='logging.yaml',
    default_level=logging.INFO,
    env_key='LOG_CFG'
):
    """Setup logging configuration

    """
    path = default_path
    value = os.getenv(env_key, None)
    if value:
        path = value
    if os.path.exists(path):
        with open(path, 'rt') as f:
            config = yaml.safe_load(f.read())
        logging.config.dictConfig(config)
    else:
        logging.basicConfig(level=default_level)
```



**Use rotating file handler**

`RotatingFileHandler` instead of `FileHandler` in production environment.



**Setup a central log server when you have multiple servers**



**Structured Logging**

Previous assumptions:

- Human read
- Programmers know what to read to debug an issue. 

Now:

you don’t write hard-to-parse and hard-to-keep-consistent prose in your logs but that you log *events* that happen in a *context* instead.



Creating new loggers with a new context:

a pre-build object which denotes all background of a process. 



**Where to configure logging?**

Ideally as late as possible but *before* non-framework (i.e. your) code is executed.



Django: the bottom of setting.

Flask: app.logger





Don't configure all loggers at once in the [`logging`](https://docs.python.org/3/library/logging.html#module-logging) package, because you could have multiple separate applications running side by side in the same Python interpreter and then it becomes impossible to have different logging setups for those.

Thus, just configure the loggers you are interested in:

```python
from logging import getLogger
loggers = [app.logger, getLogger('sqlalchemy'),
           getLogger('otherlibrary')]
for logger in loggers:
    logger.addHandler(mail_handler)
    logger.addHandler(file_handler)
```



**Error Logging Tools**

 [Sentry](http://www.getsentry.com/) 

```python
from raven.contrib.flask import Sentry
sentry = Sentry(app, dsn='YOUR_DSN_HERE')

def create_app():
    app = Flask(__name__)
    sentry.init_app(app)
    ...
    return app
```



Email logging if error

```python
ADMINS = ['yourname@example.com']
if not app.debug:
    import logging
    from logging.handlers import SMTPHandler
    mail_handler = SMTPHandler('127.0.0.1',
                               'server-error@example.com',
                               ADMINS, 'YourApplication Failed')
    mail_handler.setLevel(logging.ERROR)
    app.logger.addHandler(mail_handler)
```

